{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43d3b6-b87a-49a2-92c0-85fee4e6926e",
   "metadata": {},
   "source": [
    "# Introduction to Model Distillation: Efficient Knowledge Transfer for AI Applications - Part 2\n",
    "\n",
    "Our dataset for fine-tuning is created! We can now procede to fine-tuning - the most exciting part for most of AI developers :-).\n",
    "\n",
    "## Pre requisites\n",
    "\n",
    "This notebook picks off where [1_generate_training_data.ipynb](1_generate_training_data.ipynb) notebook finished.\n",
    "\n",
    "Be sure to run that notebook before starting on this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8fe8e",
   "metadata": {},
   "source": [
    "## 1 - Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08124040-bef8-47cf-8fb7-5860ede4b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Sequence\n",
    "from openai import Client\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8551e9",
   "metadata": {},
   "source": [
    "## 2 - Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73c9c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66993377",
   "metadata": {},
   "source": [
    "## 3  - Initialize Client\n",
    "\n",
    "The cell below creates the OpenAI-like Client to work with Nebius AI Studio and defines necessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6a4dba-e97b-4e0c-82b0-225758ec6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_CACHE_DIR = 'cache'\n",
    "BASE_URL = \"https://api.studio.nebius.ai\"\n",
    "\n",
    "client = Client(\n",
    "    base_url=f'{BASE_URL}/v1',\n",
    "    api_key=os.getenv('NEBIUS_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae760591-ad33-4367-8bd3-a417cc51a01e",
   "metadata": {},
   "source": [
    "## 4 - Prepare fine tuning dataset\n",
    "\n",
    "This dataset was created by previous setp [1_generate_training_data.ipynb](1_generate_training_data.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afe4a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 22092\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load finetuning dataset from jsonl files\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ft_dataset = Dataset.from_json('data/ft_dataset.jsonl')\n",
    "ft_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee0699-658d-4324-bf38-9e1be647ca5f",
   "metadata": {},
   "source": [
    "First, let's split our dataset into training and validation parts. As mentioned above, we'll leave 21k obervations for training, allocating ~ 5% of observations to validate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04325d12-5bac-42f2-8caa-81fe9ed59e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 20995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 1097\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_size = 1097\n",
    "seed = 42\n",
    "\n",
    "ft_dataset_split = ft_dataset.train_test_split(test_size=validation_size, seed=seed, shuffle=True)\n",
    "ft_dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db3070-1604-42f6-8d42-7e2d997f0088",
   "metadata": {},
   "source": [
    "## 5 - Create training and validation dataset \n",
    "\n",
    "We now need to format our subsets and store them into separate files.\n",
    "\n",
    "While fine-tuning allows to avoid the usage of long detailed prompt, which, in turn, reduces the token consumption and accelerates the model inference, providing general instructions is still highly recommended. This will prevent large gradient updates in the first steps of the fine-tuning because the output will be somewhat \"expected\" by the model. Large gradient updates generally lead model weights away from local minima and result in lower quality of the model after fine-tuning.\n",
    "\n",
    "Furthermore, after pre-training, Qwen3 models underwent a large-scale 4-phased training, and each of the phases ensured the assistant's message starts with thinking part (for non-thinking generations, this thinking part is empty). Consequently, similar to the system prompt, let's include the empty thinking part into assistant's messages to avoid large gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21624d78-1b0b-456c-914f-6bd17b4b649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_fine_tuning = \"Please correct the grammar in the user's text if necessary.\"\n",
    "empty_reasoning_prefix = \"\"\"\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "\"\"\".lstrip()\n",
    "\n",
    "ft_train_save_path = 'data/fine_tuning_train.jsonl'\n",
    "ft_validation_save_path = 'data/fine_tuning_validation.jsonl'\n",
    "\n",
    "with open(ft_train_save_path, 'w') as f:\n",
    "    for inst in ft_dataset_split['train']:\n",
    "        dict_to_write = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_fine_tuning,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": inst[\"input\"],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": empty_reasoning_prefix + inst[\"output\"],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        json.dump(dict_to_write, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd448764-4000-4153-81a8-ee263c3568d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_validation_save_path, 'w') as f:\n",
    "    for inst in ft_dataset_split['test']:\n",
    "        dict_to_write = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_fine_tuning,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": inst[\"input\"],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": empty_reasoning_prefix + inst[\"output\"],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        json.dump(dict_to_write, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d8888c-6d34-4543-bea3-7538e8b46394",
   "metadata": {},
   "source": [
    "After both files are created, upload them to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab2118c-c612-4d17-937c-45665a30414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-76807105-7fed-4350-940c-a1dd1b5b6f49', bytes=8802754, created_at=1753341501, filename='fine_tuning_train.jsonl', object='file', purpose='fine-tune', status=None, expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_train_file = client.files.create(\n",
    "    file=open(ft_train_save_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "fine_tuning_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349830ed-1065-41a4-80f3-3cd3e95410b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-d1c98e51-a4ab-4d9d-9c2d-5e0a1b800f20', bytes=454502, created_at=1753341502, filename='fine_tuning_validation.jsonl', object='file', purpose='fine-tune', status=None, expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_validation_file = client.files.create(\n",
    "    file=open(ft_validation_save_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "fine_tuning_validation_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835972b-42f8-4d72-b04e-cab687751f45",
   "metadata": {},
   "source": [
    "## 6 - Fine tuning\n",
    "\n",
    "\n",
    "_Heads up: Running this part will cost ~$7.1_\n",
    "\n",
    "### 6.1 - Launch fine tuning job!\n",
    "\n",
    "We are ready to launch the fine-tuning. \n",
    "\n",
    "- We'll train LoRA adapters to reduce the usage price of the model. \n",
    "- We slightly increase the rank of the LoRA (`lora_r` parameter) to reduce the quality gap between the full fine-tuning and fine-tuning of LoRA adapters. \n",
    "- We also increase the LoRA alpha value correspondingly to keep the ratio of `lora_r` to `lora_alpha` equal to 1, as suggested in the original LoRA paper [3].\n",
    "- Since our inputs and outputs are pretty short, we can use the maximum available batch size (32). \n",
    "- We will train the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb5c5d8-7ce4-451d-bec4-c6fe8a32a710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-c7cc1f8df48144aeb180a5bf5818dc81', created_at=1753341502, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=32, learning_rate_multiplier=None, n_epochs=10, learning_rate=1e-05, warmup_ratio=0.0, weight_decay=0.0, lora=True, lora_r=16, lora_alpha=16, lora_dropout=0.0, packing=True, max_grad_norm=1.0, context_length=8192), model='Qwen/Qwen3-4B', object='fine_tuning.job', organization_id='', result_files=[], seed=42, status='running', trained_tokens=0, training_file='file-76807105-7fed-4350-940c-a1dd1b5b6f49', validation_file='file-d1c98e51-a4ab-4d9d-9c2d-5e0a1b800f20', estimated_finish=None, integrations=None, metadata=None, method=None, suffix='', trained_steps=0, total_steps=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_job = client.fine_tuning.jobs.create(\n",
    "    training_file=fine_tuning_train_file.id,\n",
    "    validation_file=fine_tuning_validation_file.id,\n",
    "    model=\"Qwen/Qwen3-4B\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"lora\": True,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"packing\": True\n",
    "    },\n",
    "    seed=42\n",
    ")\n",
    "ft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c5df2-0f00-460f-a1ee-520484e2b54b",
   "metadata": {},
   "source": [
    "## 6.2 - Monitor job progress\n",
    "\n",
    "Same as with batched generation, this process may take some time. The loop below will update the state of the fine-tuning job every minute and stop once it is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0b0090-988e-4831-8f11-93957ec2424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning job ftjob-c7cc1f8df48144aeb180a5bf5818dc81 created, waiting for completion...\n",
      "Elapsed: 60s (1.0 min) : current status: running\n",
      "Elapsed: 121s (2.0 min) : current status: running\n",
      "Elapsed: 182s (3.0 min) : current status: running\n",
      "Elapsed: 242s (4.0 min) : current status: running\n",
      "Elapsed: 303s (5.1 min) : current status: running\n",
      "Elapsed: 364s (6.1 min) : current status: running\n",
      "Elapsed: 425s (7.1 min) : current status: running\n",
      "Elapsed: 485s (8.1 min) : current status: running\n",
      "Elapsed: 546s (9.1 min) : current status: running\n",
      "Elapsed: 607s (10.1 min) : current status: running\n",
      "Elapsed: 668s (11.1 min) : current status: running\n",
      "Elapsed: 728s (12.1 min) : current status: running\n",
      "Elapsed: 789s (13.2 min) : current status: running\n",
      "Elapsed: 850s (14.2 min) : current status: succeeded\n",
      "CPU times: user 199 ms, sys: 23.4 ms, total: 222 ms\n",
      "Wall time: 14min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "update_num_seconds = 60\n",
    "active_statuses = [\"validating_files\", \"queued\", \"running\"]\n",
    "print (f\"Fine tuning job {ft_job.id} created, waiting for completion...\")\n",
    "\n",
    "while ft_job.status in active_statuses:\n",
    "    time.sleep(update_num_seconds)\n",
    "    ft_job = client.fine_tuning.jobs.retrieve(ft_job.id)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Elapsed: {int(elapsed)}s ({elapsed/60:.1f} min) : current status: {ft_job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e7ed7-b2bb-40b0-b127-552949c5a7cb",
   "metadata": {},
   "source": [
    "## 7 - Inspect training quality\n",
    "\n",
    "Let's examine the loss on the validation set on each epoch to download the checkpoint yielding the highest quality.\n",
    "\n",
    "### 7.1 - Visualize loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "712ad9ce-22b6-4108-9cc0-cb86502ffbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b3d12_row0_col0, #T_b3d12_row0_col1 {\n",
       "  background-color: #67000d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b3d12_row1_col0 {\n",
       "  background-color: #fb6b4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b3d12_row1_col1 {\n",
       "  background-color: #fdccb8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row2_col0 {\n",
       "  background-color: #fcbda4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row2_col1 {\n",
       "  background-color: #fedbcc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row3_col0, #T_b3d12_row4_col1 {\n",
       "  background-color: #fee1d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row3_col1 {\n",
       "  background-color: #fedccd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row4_col0 {\n",
       "  background-color: #ffebe2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row5_col0 {\n",
       "  background-color: #fff0e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row5_col1 {\n",
       "  background-color: #ffece4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row6_col0 {\n",
       "  background-color: #fff3ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row6_col1 {\n",
       "  background-color: #fdd7c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row7_col0 {\n",
       "  background-color: #fff4ef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row7_col1, #T_b3d12_row8_col0, #T_b3d12_row9_col0 {\n",
       "  background-color: #fff5f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row8_col1 {\n",
       "  background-color: #fee9df;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b3d12_row9_col1 {\n",
       "  background-color: #fedecf;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b3d12\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b3d12_level0_col0\" class=\"col_heading level0 col0\" >train_loss</th>\n",
       "      <th id=\"T_b3d12_level0_col1\" class=\"col_heading level0 col1\" >valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b3d12_row0_col0\" class=\"data row0 col0\" >2.114447</td>\n",
       "      <td id=\"T_b3d12_row0_col1\" class=\"data row0 col1\" >0.416740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b3d12_row1_col0\" class=\"data row1 col0\" >1.149326</td>\n",
       "      <td id=\"T_b3d12_row1_col1\" class=\"data row1 col1\" >0.241002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b3d12_row2_col0\" class=\"data row2 col0\" >0.662357</td>\n",
       "      <td id=\"T_b3d12_row2_col1\" class=\"data row2 col1\" >0.230296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b3d12_row3_col0\" class=\"data row3 col0\" >0.423568</td>\n",
       "      <td id=\"T_b3d12_row3_col1\" class=\"data row3 col1\" >0.229275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b3d12_row4_col0\" class=\"data row4 col0\" >0.305986</td>\n",
       "      <td id=\"T_b3d12_row4_col1\" class=\"data row4 col1\" >0.226082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b3d12_row5_col0\" class=\"data row5 col0\" >0.247930</td>\n",
       "      <td id=\"T_b3d12_row5_col1\" class=\"data row5 col1\" >0.210415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b3d12_row6_col0\" class=\"data row6 col0\" >0.218345</td>\n",
       "      <td id=\"T_b3d12_row6_col1\" class=\"data row6 col1\" >0.233326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b3d12_row7_col0\" class=\"data row7 col0\" >0.203119</td>\n",
       "      <td id=\"T_b3d12_row7_col1\" class=\"data row7 col1\" >0.199091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b3d12_row8_col0\" class=\"data row8 col0\" >0.195031</td>\n",
       "      <td id=\"T_b3d12_row8_col1\" class=\"data row8 col1\" >0.214885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3d12_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b3d12_row9_col0\" class=\"data row9 col0\" >0.190383</td>\n",
       "      <td id=\"T_b3d12_row9_col1\" class=\"data row9 col1\" >0.228618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x71769e559bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_checkpoints = client.fine_tuning.jobs.checkpoints.list(ft_job.id).data\n",
    "metrics = []\n",
    "for epoch_data in ft_checkpoints:\n",
    "    epoch_metrics = {}\n",
    "    epoch_metrics[\"train_loss\"] = epoch_data.metrics.train_loss\n",
    "    epoch_metrics[\"valid_loss\"] = epoch_data.metrics.valid_loss\n",
    "    metrics.append(epoch_metrics)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics.style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018e0db-ff25-4967-af24-91f6ea5a69b8",
   "metadata": {},
   "source": [
    "### 7.2 - save the last checkpoint\n",
    "\n",
    "We can see our loss on the validation set has been gradually decreasing - meaning that, most likely, we could train our adapters even further to squeeze out the best quality. Therefore, let's save the last trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60f99f7-95ed-4e68-a921-015c56c3c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "save_dir = \"models/qwen3-4b-grammar-checker\"\n",
    "shutil.rmtree(save_dir, ignore_errors=True)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "n_selected_epoch = -1\n",
    "best_checkpoint = ft_checkpoints[n_selected_epoch]\n",
    "\n",
    "for model_file_id in best_checkpoint.result_files:\n",
    "    # Get the name of the file\n",
    "    file_name = client.files.retrieve(model_file_id).filename.split('/')[1]\n",
    "    # Retrieve the contents of the file\n",
    "    file_content = client.files.content(model_file_id)\n",
    "    # Save the file\n",
    "    file_content.write_to_file(os.path.join(save_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234edd69-bea8-4c36-ba36-8143845b919e",
   "metadata": {},
   "source": [
    "## 8 - How much did our fine tuning cost?\n",
    "\n",
    "The price for fine-tuning a model under 20B parameters is \\\\$0.4/1M tokens. Let's calculate the total fine-tuning price.\n",
    "\n",
    "[pricing guide](https://nebius.com/prices-ai-studio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4bac6c7-6c13-4188-b227-d984007f8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning price: $7.3\n"
     ]
    }
   ],
   "source": [
    "price = ft_job.trained_tokens * 0.4 / 1_000_000\n",
    "print(f'Fine-tuning price: ${price:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865be759-d4c2-4adb-ae5a-30858c817f3b",
   "metadata": {},
   "source": [
    "## 9 -  Deploy the model in AI Studio\n",
    "\n",
    "Nebius AI Studio provides a \"Zero-click deployment\" feature, which enables automatic deployment of trained LoRA adapters to AI Studio inference platform, empowering seamless use of your trained model for inference.\n",
    "\n",
    "The list of models supported for integration of fine-tuning and inference is provided here: https://docs.nebius.com/studio/fine-tuning/models#deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67cb4642-fb6c-4947-b22c-06875c089385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Qwen/Qwen3-4B-fast-LoRa:grammar-checker-WpCf',\n",
       " 'base_model': 'Qwen/Qwen3-4B-fast',\n",
       " 'source': 'ftjob-c7cc1f8df48144aeb180a5bf5818dc81:ftckpt_9d1ea66e-4e86-4785-9c88-372be3ea1b73',\n",
       " 'description': 'Qwen-3-4B model fine-tuned on the grammatic error correction task.',\n",
       " 'created_at': 1753342377,\n",
       " 'status': 'validating'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_creation_request = {\n",
    "    \"name\": \"grammar-checker\",  # You can set whatever name you like\n",
    "    \"base_model\": \"Qwen/Qwen3-4B-fast\",  # Base model. Qwen3-4B is only available with the `fast` mode\n",
    "    \"source\": f\"{ft_job.id}:{best_checkpoint.id}\",\n",
    "    \"description\": \"Qwen-3-4B model fine-tuned on the grammatic error correction task.\"\n",
    "}\n",
    "url = f\"{BASE_URL}/v0/models\"\n",
    "\n",
    "response = requests.post(\n",
    "    url,\n",
    "    json=lora_creation_request,\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('NEBIUS_API_KEY')}\"\n",
    "    }\n",
    ")    \n",
    "response_json = response.json()\n",
    "model_name = response_json[\"name\"]\n",
    "\n",
    "# save the model name to a file so next notebook can use it\n",
    "with open('model_name.out', 'w') as f:\n",
    "    f.write(model_name)\n",
    "    \n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f6410-fcad-4abf-a77b-0c2ccf2c0963",
   "metadata": {},
   "source": [
    "We need to wait for a few seconds or minutes until it's deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2928254-1528-4c26-8672-4eab3e0b3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: active\n"
     ]
    }
   ],
   "source": [
    "url = f\"{BASE_URL}/v0/models/{model_name}\"\n",
    "active_statuses = [\"validating\"]\n",
    "update_num_seconds = 15\n",
    "\n",
    "while response_json['status'] in active_statuses:\n",
    "    time.sleep(update_num_seconds)\n",
    "    response = requests.get(\n",
    "        url, \n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.getenv('NEBIUS_API_KEY')}\"\n",
    "        }\n",
    "    )\n",
    "    response_json = response.json()\n",
    "    print(\"Current status:\", response_json['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb513c",
   "metadata": {},
   "source": [
    "## 10 - See our model on AI Studio\n",
    "\n",
    "You can see the custom model under **models --> custom** section\n",
    "\n",
    "![](distilled-model-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3151a",
   "metadata": {},
   "source": [
    "## Done 👏"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
